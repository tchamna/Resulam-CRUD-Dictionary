name: Deploy to EC2

on:
  push:
    branches: [main]
  workflow_dispatch:

concurrency:
  group: deploy-ec2
  cancel-in-progress: false

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Create SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY || vars.EC2_SSH_KEY }}" > ~/.ssh/ec2-key.pem
          chmod 600 ~/.ssh/ec2-key.pem
          ssh-keyscan -H ${{ secrets.EC2_HOST || vars.EC2_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy on EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST || vars.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER || vars.EC2_USER || 'ec2-user' }}
          APP_DIR: ${{ secrets.APP_DIR || vars.APP_DIR || '/home/ec2-user/apps/resulam_dictionary' }}
          REPO_URL: https://github.com/${{ github.repository }}.git
          DATABASE_URL: ${{ secrets.DATABASE_URL || vars.DATABASE_URL }}
          JWT_SECRET: ${{ secrets.JWT_SECRET || vars.JWT_SECRET }}
          ACCESS_TOKEN_EXPIRES_MIN: ${{ secrets.ACCESS_TOKEN_EXPIRES_MIN || vars.ACCESS_TOKEN_EXPIRES_MIN || '0' }}
          REFRESH_TOKEN_EXPIRES_DAYS: ${{ secrets.REFRESH_TOKEN_EXPIRES_DAYS || vars.REFRESH_TOKEN_EXPIRES_DAYS || '0' }}
          SUPER_ADMIN_EMAIL: ${{ secrets.SUPER_ADMIN_EMAIL || vars.SUPER_ADMIN_EMAIL }}
          SUPER_ADMIN_PASSWORD: ${{ secrets.SUPER_ADMIN_PASSWORD || vars.SUPER_ADMIN_PASSWORD }}
          WORD_LIST_PATH: ${{ secrets.WORD_LIST_PATH || vars.WORD_LIST_PATH || 'nufi_word_list.txt' }}
          APP_BASE_URL: ${{ secrets.APP_BASE_URL || vars.APP_BASE_URL || 'https://example.com' }}
          SMTP_HOST: ${{ secrets.SMTP_HOST || vars.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT || vars.SMTP_PORT || '587' }}
          SMTP_USER: ${{ secrets.SMTP_USER || vars.SMTP_USER }}
          SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD || vars.SMTP_PASSWORD }}
          SMTP_FROM: ${{ secrets.SMTP_FROM || vars.SMTP_FROM }}
          SMTP_USE_TLS: ${{ secrets.SMTP_USE_TLS || vars.SMTP_USE_TLS || 'true' }}
          GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID || vars.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET || vars.GOOGLE_CLIENT_SECRET }}
          GOOGLE_REDIRECT_URI: ${{ secrets.GOOGLE_REDIRECT_URI || vars.GOOGLE_REDIRECT_URI }}
          AWS_REGION: ${{ secrets.AWS_DEFAULT_REGION || vars.AWS_DEFAULT_REGION || 'us-east-1' }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID || vars.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY || vars.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET: ${{ secrets.S3_BUCKET || vars.S3_BUCKET }}
          S3_PREFIX: ${{ secrets.S3_PREFIX || vars.S3_PREFIX || 'crud_dictionaries' }}
          S3_AUTO_BACKUP_ENABLED: ${{ secrets.S3_AUTO_BACKUP_ENABLED || vars.S3_AUTO_BACKUP_ENABLED || 'true' }}
          S3_AUTO_BACKUP_MIN_INTERVAL_SEC: ${{ secrets.S3_AUTO_BACKUP_MIN_INTERVAL_SEC || vars.S3_AUTO_BACKUP_MIN_INTERVAL_SEC || '0' }}
          S3_SNAPSHOT_RETENTION: ${{ secrets.S3_SNAPSHOT_RETENTION || vars.S3_SNAPSHOT_RETENTION || '1' }}
          S3_SQLITE_SNAPSHOT_ENABLED: ${{ secrets.S3_SQLITE_SNAPSHOT_ENABLED || vars.S3_SQLITE_SNAPSHOT_ENABLED || 'true' }}
          S3_SQLITE_SNAPSHOT_RETENTION: ${{ secrets.S3_SQLITE_SNAPSHOT_RETENTION || vars.S3_SQLITE_SNAPSHOT_RETENTION || '1' }}
          CRON_SCHEDULE: ${{ secrets.BACKUP_CRON || vars.BACKUP_CRON || '*/5 * * * *' }}
        run: |
          set -euo pipefail
          env_file="$(mktemp)"
          cat > "$env_file" <<EOF
          DATABASE_URL=$DATABASE_URL
          JWT_SECRET=$JWT_SECRET
          ACCESS_TOKEN_EXPIRES_MIN=$ACCESS_TOKEN_EXPIRES_MIN
          REFRESH_TOKEN_EXPIRES_DAYS=$REFRESH_TOKEN_EXPIRES_DAYS
          SUPER_ADMIN_EMAIL=$SUPER_ADMIN_EMAIL
          SUPER_ADMIN_PASSWORD=$SUPER_ADMIN_PASSWORD
          WORD_LIST_PATH=$WORD_LIST_PATH
          APP_BASE_URL=$APP_BASE_URL
          SMTP_HOST=$SMTP_HOST
          SMTP_PORT=$SMTP_PORT
          SMTP_USER=$SMTP_USER
          SMTP_PASSWORD=$SMTP_PASSWORD
          SMTP_FROM=$SMTP_FROM
          SMTP_USE_TLS=$SMTP_USE_TLS
          GOOGLE_CLIENT_ID=$GOOGLE_CLIENT_ID
          GOOGLE_CLIENT_SECRET=$GOOGLE_CLIENT_SECRET
          GOOGLE_REDIRECT_URI=$GOOGLE_REDIRECT_URI
          AWS_REGION=$AWS_REGION
          AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
          AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
          S3_BUCKET=$S3_BUCKET
          S3_PREFIX=$S3_PREFIX
          S3_AUTO_BACKUP_ENABLED=$S3_AUTO_BACKUP_ENABLED
          S3_AUTO_BACKUP_MIN_INTERVAL_SEC=$S3_AUTO_BACKUP_MIN_INTERVAL_SEC
          S3_SNAPSHOT_RETENTION=$S3_SNAPSHOT_RETENTION
          S3_SQLITE_SNAPSHOT_ENABLED=$S3_SQLITE_SNAPSHOT_ENABLED
          S3_SQLITE_SNAPSHOT_RETENTION=$S3_SQLITE_SNAPSHOT_RETENTION
          EOF
          if [ "$S3_PREFIX" = "/" ]; then
            sed -i 's/^S3_PREFIX=.*/S3_PREFIX=crud_dictionaries/' "$env_file"
          fi
          ssh -i ~/.ssh/ec2-key.pem -o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o ServerAliveCountMax=10 \
            "$EC2_USER@$EC2_HOST" APP_DIR="$APP_DIR" REPO_URL="$REPO_URL" bash -s <<'SSHINIT'
          set -euo pipefail
          sudo yum -y install docker git awscli postgresql15 cronie || true
          sudo service docker start || sudo systemctl start docker
          sudo usermod -aG docker "$USER" || true
          mkdir -p "$APP_DIR"
          cd "$APP_DIR"
          if ! git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
            git init
            git remote add origin "$REPO_URL"
          fi
          git remote set-url origin "$REPO_URL"
          git fetch --prune --force origin main || {
            git update-ref -d refs/remotes/origin/main || true
            git fetch --prune --force origin main
          }
          git reset --hard origin/main
          git clean -fd -e .env || true
          SSHINIT
          scp -i ~/.ssh/ec2-key.pem -o StrictHostKeyChecking=no "$env_file" "$EC2_USER@$EC2_HOST:$APP_DIR/api_demo/.env"
          rm -f "$env_file"
          ssh -i ~/.ssh/ec2-key.pem -o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o ServerAliveCountMax=10 \
            "$EC2_USER@$EC2_HOST" APP_DIR="$APP_DIR" bash -s <<'SSHDEPLOY'
          set -euo pipefail
          cd "$APP_DIR"
          set -a
          . api_demo/.env
          set +a
          COMPOSE_DATABASE_URL="$DATABASE_URL"
          if [ -n "${DATABASE_URL:-}" ]; then
            COMPOSE_DATABASE_URL="${COMPOSE_DATABASE_URL/localhost/db}"
            COMPOSE_DATABASE_URL="${COMPOSE_DATABASE_URL/127.0.0.1/db}"
          fi
          chmod +x scripts/backup_to_s3.sh scripts/backup_walg.sh scripts/deploy/setup_backup_cron.sh scripts/deploy/setup_walg_backup_cron.sh || true
          if ! command -v docker-compose >/dev/null 2>&1 || ! docker-compose --version 2>/dev/null | grep -q 'version 1\.'; then
            sudo curl -L https://github.com/docker/compose/releases/download/1.29.2/docker-compose-Linux-x86_64 -o /usr/local/bin/docker-compose
            sudo chmod +x /usr/local/bin/docker-compose
          fi
          if command -v docker-compose >/dev/null 2>&1; then
            DATABASE_URL="$COMPOSE_DATABASE_URL" docker-compose -f api_demo/docker-compose.yml up -d --build --force-recreate db
            db_ready=false
            for i in $(seq 1 60); do
              if docker-compose -f api_demo/docker-compose.yml exec -T db pg_isready -U postgres >/dev/null 2>&1; then
                db_ready=true
                break
              fi
              sleep 2
            done
            if [ "$db_ready" != "true" ]; then
              docker-compose -f api_demo/docker-compose.yml ps
              docker-compose -f api_demo/docker-compose.yml logs db --tail 200
              exit 1
            fi
            DATABASE_URL="$COMPOSE_DATABASE_URL" docker-compose -f api_demo/docker-compose.yml up -d --build --force-recreate web
            for i in $(seq 1 5); do
              if DATABASE_URL="$COMPOSE_DATABASE_URL" docker-compose -f api_demo/docker-compose.yml exec -T web alembic upgrade head; then
                break
              fi
              sleep 5
            done
          elif docker compose version >/dev/null 2>&1; then
            DATABASE_URL="$COMPOSE_DATABASE_URL" docker compose -f api_demo/docker-compose.yml up -d --build --force-recreate db
            db_ready=false
            for i in $(seq 1 60); do
              if docker compose -f api_demo/docker-compose.yml exec -T db pg_isready -U postgres >/dev/null 2>&1; then
                db_ready=true
                break
              fi
              sleep 2
            done
            if [ "$db_ready" != "true" ]; then
              docker compose -f api_demo/docker-compose.yml ps
              docker compose -f api_demo/docker-compose.yml logs db --tail 200
              exit 1
            fi
            DATABASE_URL="$COMPOSE_DATABASE_URL" docker compose -f api_demo/docker-compose.yml up -d --build --force-recreate web
            for i in $(seq 1 5); do
              if DATABASE_URL="$COMPOSE_DATABASE_URL" docker compose -f api_demo/docker-compose.yml exec -T web alembic upgrade head; then
                break
              fi
              sleep 5
            done
          else
            echo 'docker compose is not available' >&2
            exit 1
          fi
          APP_DIR="$APP_DIR" S3_BUCKET="$S3_BUCKET" S3_PREFIX="$S3_PREFIX" DATABASE_URL="$DATABASE_URL" CRON_SCHEDULE="$CRON_SCHEDULE" \
            bash scripts/deploy/setup_backup_cron.sh
          APP_DIR="$APP_DIR" WALG_S3_PREFIX="" CRON_SCHEDULE="$CRON_SCHEDULE" \
            bash scripts/deploy/setup_walg_backup_cron.sh
          SSHDEPLOY
